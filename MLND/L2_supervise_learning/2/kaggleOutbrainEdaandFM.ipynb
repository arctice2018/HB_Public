{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大规模数据下的模型训练优化\n",
    "## 项目：OutBrain广告CTR预估\n",
    "\n",
    "竞赛背景\n",
    "\n",
    "     #Outbrain Click Prediction#\n",
    "     广告预估通常会面临超高的稀疏维度，以及超大量的训练数据。在计算资源受限的条件，如何保持训练效率和精度的平衡是CTR预估需要解决的一个主要问题。此数据集来源于Kaggle Outbrain Click Prediction竞赛。包含大量用户对于网页广告页面的点击，浏览以及广告发布人信息。给定对于页面的点击信息以及Context（上下文）相关信息，来对用户可能点击的广告页进行预测，从而达到推荐的目标。\n",
    "\n",
    "> **提示：**类似于此部分的引用部分可以提供关于如何浏览和使用 iPython notebook 的实用说明。\n",
    "\n",
    "# 开始\n",
    "处理train和test数据集，通过将数去读取生成pandas dataframe，对数据做进一步的操作和和可视化分析\n",
    "运行以下代码单元格，加载我们的数据并使用 `.head()` 函数显示前几个条目（乘客）以进行检查。\n",
    "> **提示：**你可以通过点击单元格和使用键盘快捷键 **Shift + Enter** 或 **Shift + Return** 运行代码单元格。此外，在选中单元格后还可以使用工具栏中的**播放**按钮执行代码单元格。双击 Markdown 单元格（如下所示的文本单元格）即可进行编辑，并使用相同的快捷键保存。[Markdown](http://daringfireball.net/projects/markdown/syntax) 使你能够编写易于阅读的纯文本文件，这些文件可以转换为 HTML。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from csv import DictReader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "path = \"./\"\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的读取\n",
    "* 由于数据量巨大，在线上notebook中可能无法全部加载并实现后续运算，因此此处可以选择加载少量数据进行测试\n",
    "* 只使用500万行作为代码示例，有兴趣和充足计算资源可以下载完整数据并进行测试计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## 读取数据\n",
    "sample = True\n",
    "if sample:\n",
    "    reader = pd.read_csv(path+'clicks_train_5000000.csv',iterator=True,chunksize=5000000)\n",
    "    df_train = next(reader)\n",
    "else:\n",
    "    df_train = pd.read_csv(path+'clicks_train_5000000.csv')\n",
    "\n",
    "## 分别统计ad_id的个数，以及样本的数量\n",
    "print('unique ad_id: %s'%df_train['ad_id'].nunique())\n",
    "print('训练集共有%s条样本'%(df_train.shape[0]))\n",
    "\n",
    "display_ids = df_train['display_id'].values.tolist()\n",
    "\n",
    "if 'df_train_tr.csv' not in os.listdir(path):\n",
    "    ## 切分数据集，缓存到硬盘中。供后续进行sgd训练\n",
    "    kf = KFold(n_splits=5,random_state=42,shuffle=True).split(df_train.values)\n",
    "    \n",
    "    ind_tr,ind_te = next(kf)\n",
    "    \n",
    "    df_train_tr = df_train.iloc[ind_tr]\n",
    "    df_train_val = df_train.iloc[ind_te]\n",
    "    df_train_tr.to_csv(path+'df_train_tr.csv',index=False)\n",
    "    df_train_val.to_csv(path+'df_train_val.csv',index=False)\n",
    "    \n",
    "    try:del df_train_tr,df_train_val # Being nice to Azure\n",
    "    except:pass;gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的初步分析\n",
    "* 数据字段的含义 [Outbrain CTR Prediction][1]\n",
    "[1]: (https://www.kaggle.com/c/outbrain-click-prediction/data） \"kaggle\"\n",
    "\n",
    "* 对于CTR数据，我们可以对其中相关的ad_id,display_id等字段信息进行观察，查看其稀疏程度，对于后续采用什么样的训练策略和特征工程策略有启发意义\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题0 统计在同一个display_id中出现广告的次数占总数据的比例\n",
    "在进行训练之前，需要对数据进行一些分析，我们首先检验display_id的稀疏程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计在同一个display_id中出现广告的次数占总数据的比例\n",
    "sizes_train = None\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "p = sns.color_palette()\n",
    "sns.barplot(sizes_train.index, sizes_train.values, alpha=0.8, color=p[0], label='train')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Ads in display', fontsize=12)\n",
    "plt.ylabel('Proportion of set', fontsize=12)\n",
    "\n",
    "try:del sizes_train # Being nice to Azure\n",
    "except:pass;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计ad_id出现的次数\n",
    "\n",
    "ad_unique_count = df_train['ad_id'].value_counts()\n",
    "\n",
    "## 分别统计ad_id出现次数占比\n",
    "for i in [2,5,10,50,200,500,1000]:\n",
    "    print('Ads that appear less than {} times: {}%'.format(i, round((ad_unique_count < i).mean() * 100, 2)))\n",
    "\n",
    "try:del df_train # Being nice to Azure\n",
    "except:pass;gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分析\n",
    "events.csv包含用户的点击事件，包含了用户点击平台，事件，地点，点击的文档和用户id等信息\n",
    "在这个notebook中，我们暂且不使用地点和时间信息进行特征工程\n",
    "可行的特征工程方法包括：\n",
    "* 提取时间年月日小时等\n",
    "* 切分geo location，将国家，州，区块号划分出来构建特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对event进行分析\n",
    "\n",
    "try:del df_train # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "events = pd.read_csv(path+'events_5000000.csv')\n",
    "events = events[events['display_id'].isin(display_ids)]\n",
    "print('Shape:', events.shape)\n",
    "print('Columns', events.columns.tolist())\n",
    "events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计document出现的次数\n",
    "print('统计document出现的次数')\n",
    "document_unique_count = events['document_id'].value_counts()\n",
    "document_unique_count.hist(bins=50)\n",
    "plt.show()\n",
    "## 分别统计ad_id出现次数占比\n",
    "for i in [2,5,10,50,200,500,1000]:\n",
    "    print('Documents that appear less than {} times: {}%'.format(i, round((document_unique_count < i).mean() * 100, 2)))\n",
    "\n",
    "try:del document_unique_count # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "## 统计platform出现的次数\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "print('统计platform出现的次数')\n",
    "platform_unique_count = events['platform'].value_counts()\n",
    "print(platform_unique_count)\n",
    "sns.barplot(platform_unique_count.index, platform_unique_count.values, alpha=0.8, color=p[0])\n",
    "\n",
    "try:del platform_unique_count # Being nice to Azure\n",
    "except:pass;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对documents_topics进行分析\n",
    "document_ids = events['document_id'].values.tolist()\n",
    "\n",
    "try:del df_train,df_test # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "documents_topics = pd.read_csv(path+'documents_topics.csv')\n",
    "documents_topics = documents_topics[documents_topics['document_id'].isin(document_ids)]\n",
    "\n",
    "print('Shape:', documents_topics.shape)\n",
    "print('Columns', documents_topics.columns.tolist())\n",
    "documents_topics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计topic出现的次数\n",
    "print('统计topic出现的次数')\n",
    "topic_unique_count = documents_topics['topic_id'].value_counts()\n",
    "\n",
    "topic_unique_count.hist(bins=50)\n",
    "\n",
    "## 分别统计topic出现次数占比\n",
    "for i in [10000,20000,50000,100000]:\n",
    "    print('Topics that appear less than {} times: {}%'.format(i, round((topic_unique_count < i).mean() * 100, 2)))\n",
    "\n",
    "try:del topic_unique_count # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对documents_meta.csv进行分析\n",
    "\n",
    "try:del df_train,df_test # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "documents_meta = pd.read_csv(path+'documents_meta.csv')\n",
    "documents_meta = documents_meta[documents_meta['document_id'].isin(document_ids)]\n",
    "\n",
    "print('Shape:', documents_meta.shape)\n",
    "print('Columns', documents_meta.columns.tolist())\n",
    "documents_meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计publisher_id出现的次数\n",
    "print('统计publisher_id出现的次数')\n",
    "publisher_unique_count = documents_meta['publisher_id'].value_counts()\n",
    "\n",
    "publisher_unique_count.hist(bins=50)\n",
    "\n",
    "## 分别统计publisher出现次数占比\n",
    "for i in [10,50,200,1000,5000]:\n",
    "    print('Publishers that appear less than {} times: {}%'.format(i, round((publisher_unique_count < i).mean() * 100, 2)))\n",
    "\n",
    "\n",
    "    \n",
    "try:del publisher_unique_count # Being nice to Azure\n",
    "except:pass;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对documents_categories.csv进行分析\n",
    "\n",
    "try:del df_train,df_test # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "documents_categories = pd.read_csv(path+'documents_categories.csv')\n",
    "documents_categories = documents_categories[documents_categories['document_id'].isin(document_ids)]\n",
    "\n",
    "print('Shape:', documents_categories.shape)\n",
    "print('Columns', documents_categories.columns.tolist())\n",
    "documents_categories.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计category出现的次数\n",
    "print('统计category出现的次数')\n",
    "category_unique_count = documents_categories['category_id'].value_counts()\n",
    "category_unique_count.hist(bins=50)\n",
    "\n",
    "## 分别统计category出现次数占比\n",
    "for i in [100,1000,5000,20000,100000]:\n",
    "    print('Categories that appear less than {} times: {}%'.format(i, round((category_unique_count < i).mean() * 100, 2)))\n",
    "\n",
    "try:del category_unique_count # Being nice to Azure\n",
    "except:pass;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对documents_entities.csv进行分析\n",
    "\n",
    "try:del df_train,df_test # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "documents_entities = pd.read_csv(path+'documents_entities.csv')\n",
    "documents_entities = documents_entities[documents_entities['document_id'].isin(document_ids)]\n",
    "\n",
    "print('Shape:', documents_entities.shape)\n",
    "print('Columns', documents_entities.columns.tolist())\n",
    "documents_entities.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计entity出现的次数\n",
    "print('统计entity出现的次数')\n",
    "entity_unique_count = documents_entities['entity_id'].value_counts()\n",
    "entity_unique_count.hist(bins=50)\n",
    "\n",
    "## 分别统计entity出现次数占比\n",
    "for i in [2,5,10,30]:\n",
    "    print('Categories that appear less than {} times: {}%'.format(i, round((entity_unique_count < i).mean() * 100, 2)))\n",
    "\n",
    "\n",
    "try:del entity_unique_count # Being nice to Azure\n",
    "except:pass;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对promoted_content.csv进行分析\n",
    "\n",
    "try:del df_train,df_test # Being nice to Azure\n",
    "except:pass;gc.collect()\n",
    "\n",
    "promoted_content = pd.read_csv(path+'promoted_content.csv')\n",
    "print('Shape:', promoted_content.shape)\n",
    "print('Columns', promoted_content.columns.tolist())\n",
    "promoted_content.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是Hashing\n",
    "在介绍hash trick之前，首先需要理解什么是hash table和hash操作\n",
    "散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数(hash function)，存放记录的数组叫做散列表。\n",
    "\n",
    "我们基于一种结果尽可能随机平均分布的固定函数H为每个元素安排存储位置，这样就可以避免遍历性质的线性搜索，以达到快速存取。但是由于此随机性，也必然导致一个问题就是冲突。\n",
    "所谓冲突，即两个元素通过散列函数H得到的地址相同，那么这两个元素称为“同义词”。这类似于70个人去一个有100个椅子的饭店吃饭。散列函数的计算结果是一个存储单位地址，每个存储单位称为“桶”。设一个散列表有m个桶，则散列函数的值域应为[0,m-1]。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hash trick的作用和计算方法\n",
    "了解了hash操作后，在CTR预估以及一些大规模数据训练场景下，采用hash trick能够在不对数据提前进行预处理的情况下，直接生成稀疏输入进行训练\n",
    "hash trick的效果类似onehot encoding,但相比onehot encoding需要提前计算类目特征的数量，确定需要映射的空间最大范围，hash trick可以不用关注映射的边界。如果训练过程中发现一些新增数据不曾出现在onehot的dictionary中，onehot encoding并没有特别好的方式对数据进行处理。因此hash trick对于线上大规模数据的训练就显得更加合适了。\n",
    "\n",
    "### hash的计算\n",
    "\n",
    "* 自己实现hash trick：\n",
    "    hashed_index = abs((hash(f(key,value)) % D) \n",
    "* 其中D是需要散列的最大空间值,f(key,value)代表关于输入key和value的函数，用来结合两者，最简单的做法是作为str使用key+\"_\"+value的方式将两者结合起来\n",
    "\n",
    "* 同时也可以采用sklearn相关接口 \n",
    "提示：[Feature Hasher][1] 也相关方法可以参照sklearn文档以及其中的案例\n",
    "[1]: (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) \"sklearn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1 编写hash trick函数\n",
    "* 通过编写hash trick函数，实现将原始类目样本映射到散列空间中去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trick(key,value,D=2**20):\n",
    "    idx = None\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2.1 event特征工程\n",
    "* 给定event dataframe和需要处理的特证名，购将特征dict，dict的形式为{\"display_id_\"+display_id:{\"feature_name1\":value,\"feature_name2\":value}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## events to dict\n",
    "event_dict = dict()\n",
    "event_features = ['uuid','document_id','platform']\n",
    "for idx,event in enumerate(events.to_dict('records')):\n",
    "    if idx%200000==0:\n",
    "        print('%s lines proccessed'%idx)\n",
    "    \n",
    "    event_dict['display_id_%s'%event['display_id']] = dict()\n",
    "    for k in event_features:\n",
    "        event_dict['display_id_%s'%event['display_id']][k] = event[k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2.2 document特征工程\n",
    "* 给定document相关dataframe和需要处理的特证名，购将特征dict，dict的形式为{\"document\\_id\\_\"+document_id:{\"feature_name1\":value,\"feature_name2\":value}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_document_dict(df,features= ['source_id', 'publisher_id']):\n",
    "    documents_dict = dict()\n",
    "    \n",
    "    return documents_dict\n",
    "\n",
    "\n",
    "## document_meta to dict\n",
    "documents_meta_features = ['source_id', 'publisher_id']\n",
    "documents_meta_dict = get_document_dict(documents_meta,features=documents_meta_features)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "## documents_topics to dict\n",
    "documents_topics_features = ['topic_id']\n",
    "documents_topics_dict = get_document_dict(documents_topics,features=documents_topics_features)\n",
    "\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "## documents_entities to dict\n",
    "documents_entities_features = ['entity_id']\n",
    "documents_entities_dict = get_document_dict(documents_entities,features=documents_entities_features)\n",
    "\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "## documents_categories to dict\n",
    "documents_categories_features = ['category_id']\n",
    "documents_categories_dict = get_document_dict(documents_categories,features=documents_categories_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine(FM模型)\n",
    "\n",
    "无论是用线性回归进行回归预测，还是使用逻辑回归进行分类预测，广义线性模型族通常只能够处理线性可分的数据集，而我们在解决CTR问题的过程中，实际接触到的数据绝大部分存在非线性关系。而采用SVM，NN，GBDT等算法进行非线性算法或多或少都存在数据量过大，计算资源不足够支撑这些算法的情况存在。因此如果在线性的时间复杂度内，完成一个模型的非线性特征拟合，催生出了FM模型以及其后续FFM模型的产生。\n",
    "通常情况下，能够对广义线性模型进行非线性特征拟合能力增强的方法有如下措施：\n",
    "\n",
    "    1.核函数方法或者隐层+激活函数，将非线性数据集映射到高维空间中，使其近似线性可分（相当于改造成SVM和NN）\n",
    "    2.多项式回归，认为构造特征的多级交互\n",
    "    3.FM模型，对线性模型的部分进行扩展。在保留线性部分的同时，增加非线性的隐向量（latent vector），近似二阶（或者更高，但是计算效率会随着order的提高而变差）多项式回归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式模型存在的问题\n",
    "\n",
    "在很长一段时间内，多项式回归因其高效的训练方式（online learning）和对于hash trick的良好兼容，作为业界解决方案而被广泛采用\n",
    "\n",
    "然而多项式模型也存在一定的问题\n",
    "\n",
    "先从线性回归和多项式回归开始建模，这里我们以二阶多项式模型（degree = 2时）为例：\n",
    "\n",
    "$\\hat{y}(x) := \\underbrace {w_0 + \\sum_{i=1}^{n} w_i x_i }_{\\text{线性回归}} + \\underbrace {\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} w_{ij} x_i x_j}_{\\text{交叉项（组合特征）}}$\n",
    "\n",
    "n 代表样本的特征数量，$xi$是第 i 个特征的值，$w0$、$wi$、$wij$是模型参数。由于对数据进行了2阶组合，在线性部分外，组合特征一共有n(n-1)/2个，也将对应生成n(n-1)/2 模型系数，其算法的时间复杂度变为$O(n^2)$,当n为一个很大的数字时，会造成两个问题：\n",
    "* 数据过于稀疏，特征维度很有可能远远高于样本量，很大概率造成过拟合，需要增加更强的正则\n",
    "* 增加训练和存储模型参数所需要的运算资源和存储资源\n",
    "\n",
    "### 利用FM模型解决这类问题\n",
    "Steffen Rendle于2010年提出了Factorization Machines(FM) ，相对于其他非线性模型，FM模型存在以下优势\n",
    "\n",
    "    1.FM可以实现对于输入数据是非常稀疏（比如自动推荐系统），而SVM会效果很差，因为训出的SVM模型会面临较高的bias。\n",
    "    2.FM拥有线性的复杂度\n",
    "\n",
    "FM模型中认为\n",
    "所有二次项参数 $w_{ij}$ 可以组成一个对称阵 $W$，可以分解为$W=V^TV$，$V$ 的第 j 列便是第 j 维特征的隐向量，也就是说每个参数 $w_{ij}=⟨v_i,v_j⟩w_{ij}=⟨v_i,v_j⟩$ ，这就是FM模型的核心思想（不讨论高阶形式）。\n",
    "\n",
    "所以可以得到： \n",
    "$\\hat{y}(\\mathbf{x}) := w_0 + \\sum_{i=1}^{n} w_i x_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j$\n",
    "\n",
    "##### FM的化简\n",
    "此时原始模型算法复杂度依然为$O(n^2)$，不过我们通过化简trick得到：\n",
    "$\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j = \\frac{1}{2} \\sum_{f=1}^k \\left(\\left( \\sum_{i=1}^n v_{i, f} x_i \\right)^2 - \\sum_{i=1}^n v_{i, f}^2 x_i^2 \\right)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题3 一些FM模型需要的函数\n",
    "* 3.1 矩阵的点击计算\n",
    "* 3.2 exponential loss函数$ln(1+exp(-y*p))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(u,v):\n",
    "    u_v = Noen\n",
    "    return u_v\n",
    "\n",
    "\n",
    "def exponential_loss_function(y,p):\n",
    "    loss = None\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型的数据加载器\n",
    "为了让模型使用随机梯度下降算法，我们可以构建一个数据加载器，从硬盘中逐行读取数据，配合我们先前构建的特征dict来实现特征的结合\n",
    "此处我们最终实现的是对特征的逐条生成，并且输入模型中，使用Adam——一种优化过的自适应梯度下降算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt,pow\n",
    "import itertools\n",
    "import math\n",
    "from random import random,shuffle,uniform,seed\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "seed(1024)\n",
    "\n",
    "def data_generator(path,no_norm=False,task='c',D=2**20):\n",
    "    for t, row in enumerate(DictReader(open(path), delimiter=',')):\n",
    "        # process id\n",
    "        \n",
    "        try:\n",
    "            ID=row['display_id']\n",
    "            del row['display_id']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # process clicks\n",
    "        y = 0.\n",
    "        target='clicked'#'IsClick' \n",
    "        if target in row:\n",
    "            if row[target] == '1':\n",
    "                y = 1.\n",
    "            del row[target]\n",
    "\n",
    "        # build x\n",
    "        x = []\n",
    "        for key in row:\n",
    "            value = row[key]\n",
    "            \n",
    "            # one-hot encode everything with hash trick\n",
    "            idx = hash_trick(key,value,D)\n",
    "            x.append([idx,1.0])\n",
    "        \n",
    "        ## events features\n",
    "        events = event_dict['display_id_%s'%ID]\n",
    "        document_id = events['document_id']\n",
    "        for key in event:\n",
    "            value = event[key]\n",
    "            # one-hot encode everything with hash trick\n",
    "            idx = hash_trick(str(key),str(value),D)\n",
    "            x.append([idx,1.0])\n",
    "        \n",
    "        ## document meta feature\n",
    "        documents_meta = documents_meta_dict.get('document_id_%s'%document_id,{})\n",
    "        for key in documents_meta:\n",
    "            value = documents_meta[key]\n",
    "            # one-hot encode everything with hash trick\n",
    "            idx = hash_trick(str(key),str(value),D)\n",
    "            x.append([idx,1.0])\n",
    "            \n",
    "            \n",
    "        ## document topic features\n",
    "        documents_topic = documents_topics_dict.get('document_id_%s'%document_id,{})\n",
    "        for key in documents_topic:\n",
    "            value = documents_topic[key]\n",
    "            # one-hot encode everything with hash trick\n",
    "            idx = hash_trick(str(key),str(value),D)\n",
    "            x.append([idx,1.0])\n",
    "            \n",
    "        ## documents_entity features\n",
    "        documents_entities = documents_entities_dict.get('document_id_%s'%document_id,{})\n",
    "        for key in documents_entities:\n",
    "            value = documents_entities[key]\n",
    "            # one-hot encode everything with hash trick\n",
    "            idx = hash_trick(str(key),str(value),D)\n",
    "            x.append([idx,1.0])\n",
    "            \n",
    "        ## documents_categories features\n",
    "        documents_categories = documents_categories_dict.get('document_id_%s'%document_id,{})\n",
    "        for key in documents_categories:\n",
    "            value = documents_categories[key]\n",
    "            # one-hot encode everything with hash trick\n",
    "            idx = hash_trick(str(key),str(value),D)\n",
    "            x.append([idx,1.0])\n",
    "        \n",
    "        \n",
    "        ## l2 row-wise normalization\n",
    "        if not no_norm:\n",
    "            r = 0.0\n",
    "            for i in range(len(x)):\n",
    "                r+=x[i][1]*x[i][1]\n",
    "            for i in range(len(x)):\n",
    "                x[i][1] /=r\n",
    "                \n",
    "        if task=='c':\n",
    "            if y ==0.0:\n",
    "                y = -1.0\n",
    "                \n",
    "        yield x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse_loss_function(y,p):\n",
    "    return (y - p)**2\n",
    "\n",
    "class FM(object):\n",
    "    def __init__(self,lr=0.001,momentum=0.9,nesterov=True,adam=False,l2=0.0,l2_fm=0.0,l2_bias=0.0,ini_stdev= 0.01,dropout=0.5,task='c',n_components=4,nb_epoch=5,interaction=False,no_norm=False,D=2**20,verbose=True):\n",
    "        self.W = []\n",
    "        self.V = []        \n",
    "        self.bias = uniform(-ini_stdev, ini_stdev)\n",
    "        self.n_components=n_components\n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "        self.l2_fm = l2_fm\n",
    "        self.l2_bias = l2_bias\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.adam = adam\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.ini_stdev = ini_stdev\n",
    "        self.task = task\n",
    "        self.interaction = interaction\n",
    "        self.dropout = dropout\n",
    "        self.no_norm = no_norm\n",
    "        self.D = D\n",
    "        self.verbose=verbose\n",
    "        if self.task!='c':\n",
    "            self.loss_function = mse_loss_function\n",
    "        else:\n",
    "            self.loss_function = exponential_loss_function\n",
    "        self.preload()\n",
    "        if adam:\n",
    "            self.adam_init()\n",
    "        \n",
    "        \n",
    "    def preload(self):\n",
    "        dim = self.D\n",
    "        print(\"Number of features:\",dim)\n",
    "\n",
    "        self.W = [uniform(-self.ini_stdev, self.ini_stdev) for _ in range(dim)]\n",
    "        self.Velocity_W = [0.0 for _ in range(dim)]\n",
    "        \n",
    "\n",
    "        self.V = [[uniform(-self.ini_stdev, self.ini_stdev) for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.Velocity_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "\n",
    "        self.Velocity_bias = 0.0\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "\n",
    "    def adam_init(self):\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.decay = 0.\n",
    "        self.inital_decay = self.decay \n",
    "\n",
    "        dim =self.dim\n",
    "\n",
    "        self.m_W = [0.0 for _ in range(dim)]\n",
    "        self.v_W = [0.0 for _ in range(dim)]\n",
    "\n",
    "        self.m_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.v_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "\n",
    "        self.m_bias = 0.0\n",
    "        self.v_bias = 0.0\n",
    "\n",
    "\n",
    "    def adam_update(self,lr,x,residual):\n",
    "\n",
    "        if 0.<self.dropout<1.:\n",
    "            self.droupout_x(x)\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.inital_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "\n",
    "        lr_t = lr * sqrt(1. - pow(self.beta_2, t)) / (1. - pow(self.beta_1, t))\n",
    "        \n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            g = residual*value\n",
    "\n",
    "            m = self.m_W[idx]\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "            v = self.v_W[idx]\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "            p = self.W[idx]\n",
    "            p_t = p - lr_t *m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            if self.l2>0:\n",
    "                pt = pt - lr_t*self.l2*p\n",
    "\n",
    "            self.m_W[idx] = m_t\n",
    "            self.v_W[idx] = v_t\n",
    "            self.W[idx] = p_t\n",
    "\n",
    "        if self.interaction:\n",
    "            self._adam_update_fm(lr_t,x,residual)\n",
    "\n",
    "\n",
    "        m = self.m_bias\n",
    "        m_t = (self.beta_1 * m) + (1. - self.beta_1)*residual\n",
    "\n",
    "        v = self.v_bias\n",
    "        v_t = (self.beta_2 * v) + (1. - self.beta_2)*(residual**2)\n",
    "\n",
    "        p = self.bias\n",
    "        p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "        if self.l2_bias>0:\n",
    "            pt = pt - lr_t * self.l2_bias*p\n",
    "\n",
    "        self.m_bias = m_t\n",
    "        self.v_bias = v_t\n",
    "        self.bias = p_t\n",
    "\n",
    "        self.iterations+=1\n",
    "\n",
    "    def _adam_update_fm(self,lr_t,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                v = self.V[idx_i][f]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                g = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "\n",
    "                m = self.m_V[idx_i][f]\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "                v = self.v_V[idx_i][f]\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "                p = self.V[idx_i][f]\n",
    "                p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                if self.l2_fm>0:\n",
    "                    pt = pt - lr_t * self.l2_fm*p\n",
    "\n",
    "                self.m_V[idx_i][f] = m_t\n",
    "                self.v_V[idx_i][f] = v_t\n",
    "                self.V[idx_i][f] = p_t\n",
    "\n",
    "    def droupout_x(self,x):\n",
    "        new_x = []\n",
    "        for i, var in enumerate(x):\n",
    "            if random() > self.dropout:\n",
    "                del x[i]\n",
    "\n",
    "    def _predict_fm(self,x):\n",
    "        len_x = len(x)\n",
    "        n_components = self.n_components\n",
    "        pred = 0.0\n",
    "        self.sum_f_dict = {}\n",
    "        for f in range(n_components):\n",
    "            sum_f = 0.0\n",
    "            sum_sqr_f = 0.0\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                d = self.V[idx_i][f] * value_i\n",
    "                sum_f +=d\n",
    "                sum_sqr_f +=d*d\n",
    "            pred+= 0.5 * (sum_f*sum_f - sum_sqr_f);\n",
    "            self.sum_f_dict[f] = sum_f\n",
    "        return pred\n",
    "\n",
    "    def _predict_one(self,x):\n",
    "        pred = self.bias\n",
    "        # pred = 0.0\n",
    "        for idx,value in x:\n",
    "            pred+=self.W[idx]*value\n",
    "        \n",
    "        if self.interaction:\n",
    "            pred+=self._predict_fm(x)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _update_fm(self,lr,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                v = self.V[idx_i][f]\n",
    "                grad = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "                \n",
    "                self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                if self.nesterov:\n",
    "                    self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                self.V[idx_i][f] = self.V[idx_i][f] + self.Velocity_V[idx_i][f] - self.l2_fm*self.V[idx_i][f]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self,lr,x,residual):\n",
    "\n",
    "        if 0.<self.dropout<1.:\n",
    "            self.droupout_x(x)\n",
    "\n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            grad = residual*value\n",
    "            self.Velocity_W[idx] =  self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            if self.nesterov:\n",
    "                 self.Velocity_W[idx] = self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            self.W[idx] = self.W[idx] + self.Velocity_W[idx] - self.l2*self.W[idx]\n",
    "            \n",
    "        if self.interaction:\n",
    "            self._update_fm(lr,x,residual)\n",
    "\n",
    "        self.Velocity_bias = self.momentum*self.Velocity_bias - lr*residual\n",
    "        if self.nesterov:\n",
    "            self.Velocity_bias = self.momentum*self.Velocity_bias - lr*residual\n",
    "        self.bias = self.bias +self.Velocity_bias - self.l2_bias*self.bias\n",
    "\n",
    "    def predict(self,path,out):\n",
    "\n",
    "        data = data_generator(path,self.no_norm,self.task,self.D)\n",
    "        y_preds =[]\n",
    "        with open(out, 'w') as outfile:\n",
    "            ID = 0\n",
    "            outfile.write('%s,%s\\n' % ('ID', 'target'))\n",
    "            for d in data:\n",
    "                x,y = d\n",
    "                p = self._predict_one(x)\n",
    "                outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "                ID+=1\n",
    "\n",
    "\n",
    "    def validate(self,path):\n",
    "        data = data_generator(path,self.no_norm,self.task,self.D)\n",
    "        loss = 0.0\n",
    "        count = 0.0\n",
    "\n",
    "        for d in data:\n",
    "            x,y = d\n",
    "            p = self._predict_one(x)\n",
    "            loss+=self.loss_function(y,p)\n",
    "            count+=1\n",
    "        return loss/count\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = []\n",
    "        weights.append(self.W)\n",
    "        weights.append(self.V)\n",
    "        weights.append(self.bias)\n",
    "        weights.append(self.Velocity_W)\n",
    "        weights.append(self.Velocity_V)\n",
    "        weights.append(self.dim)\n",
    "        pickle.dump(weights,open('sgd_fm.pkl','wb'))\n",
    "\n",
    "    def load_weights(self):\n",
    "        weights = pickle.load(open('sgd_fm.pkl','rb'))\n",
    "        self.W = weights[0]\n",
    "        self.V = weights[1]\n",
    "        self.bias = weights[2]\n",
    "        self.Velocity_W = weights[3]\n",
    "        self.Velocity_V = weights[4]\n",
    "        self.dim = weights[5]\n",
    "        \n",
    "\n",
    "    def train(self,path,valid_path = None,in_memory=False):\n",
    "\n",
    "        start = datetime.now()\n",
    "        lr = self.lr\n",
    "        if self.adam:\n",
    "            self.adam_init()\n",
    "            self.update = self.adam_update\n",
    "\n",
    "        if in_memory:\n",
    "            data = data_generator(path,self.no_norm,self.task,self.D)\n",
    "            data = [d for d in data]\n",
    "        best_loss = 999999\n",
    "        best_epoch = 0\n",
    "        for epoch in range(1,self.nb_epoch+1):\n",
    "            if not in_memory:\n",
    "                data = data_generator(path,self.no_norm,self.task,self.D)\n",
    "            train_loss = 0.0\n",
    "            train_count = 0\n",
    "            for x,y in data:\n",
    "                p = self._predict_one(x)\n",
    "                if self.task!='c':                    \n",
    "                    residual = -(y-p)\n",
    "                else:\n",
    "                    residual = -y*(1.0-1.0/(1.0+exp(-y*p)));\n",
    "                    # residual = -(y-p)\n",
    "\n",
    "                self.update(lr,x,residual)\n",
    "                if self.verbose>0:\n",
    "                    if self.verbose==True:\n",
    "                        self.verbose = 50000\n",
    "                    if train_count%self.verbose==0:\n",
    "                        if train_count ==0:\n",
    "                            print('\\ttrain_count: %s, current loss: %.6f'%(train_count,0.0))\n",
    "                        else:\n",
    "                            print('\\ttrain_count: %s, current loss: %.6f'%(train_count,train_loss/train_count))\n",
    "                \n",
    "                train_loss += self.loss_function(y,p)\n",
    "                train_count += 1\n",
    "\n",
    "            epoch_end = datetime.now()\n",
    "            duration = epoch_end-start\n",
    "            \n",
    "            if valid_path:\n",
    "                valid_loss = self.validate(valid_path)\n",
    "                print('Epoch: %s, train loss: %.6f, valid loss: %.6f, time: %s'%(epoch,train_loss/train_count,valid_loss,duration))\n",
    "                if valid_loss<best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    print('save_weights')\n",
    "            else:\n",
    "\n",
    "                print('Epoch: %s, train loss: %.6f, time: %s'%(epoch,train_loss/train_count,duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM模型与线性模型效果对比\n",
    "\n",
    "由于FM是在线性算法的基础上增加了非线性隐变量代码，因此我们可以很方便的在代码中控制模型表现为线性模型还是FM模型\n",
    "通过interaction设置为False，模型将自动退化为逻辑回归\n",
    "\n",
    "我们使用同样的配置来进行模型的训练，以此来对比模型的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *首先是线性模型，我们使用hash trick将所有特征映射到一个2**18的向量空间中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:del fm,df_train,df_test\n",
    "except:pass;gc.collect()\n",
    "\n",
    "fm = FM(lr=0.001,adam=True,nesterov=True,task='c',nb_epoch=1,interaction=False,no_norm=False,D=2**18,verbose=1000000)\n",
    "fm.train(path+'df_train_tr.csv',path+'df_train_val.csv',in_memory=False)\n",
    "fm.predict(path+'df_train_val.csv',out=path+'valid.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *FM模型interaction=True，我们通过设置n_components=2，赋予模型隐变量维度K为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:del fm\n",
    "except:pass;gc.collect()\n",
    "\n",
    "fm = FM(lr=0.001,adam=True,nesterov=True,task='c',n_components=2,nb_epoch=1,interaction=True,no_norm=False,D=2**18,verbose=1000000)\n",
    "fm.train(path+'df_train_tr.csv',path+'df_train_val.csv',in_memory=False)\n",
    "fm.predict(path+'df_train_val.csv',out=path+'valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，使用FM模型在验证的精度上大幅超过了逻辑回归的效果，同时训练速度基本上还维持在线性的时间复杂度上，证明我们采用FM模型是一种行之有效的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "CTR相关任务通常涉及海量的数据，高维且稀疏的特征集。如果选择更好的训练策略，模型，特征工程方式将会极大的影响训练效率和精度的平衡。\n",
    "如今CTR的发展也变得更加多样，在一些大型企业中也会采用深度学习的方式，如DeepFM模型，Wide And Deep Neural Network等。但是原则上都会在训练效率和精度上取得一个平衡点，并且需要根据企业自身的资源和能力，进行适当的调节。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
