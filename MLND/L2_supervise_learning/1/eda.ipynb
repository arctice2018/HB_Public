{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 特征的探索性数据分析\n",
    "## 项目：第二届智慧中国杯（ICC） 游戏玩家消费金额预测\n",
    "\n",
    "竞赛背景\n",
    "\n",
    "      #第二届智慧中国杯（ICC）#\n",
    "\n",
    "     《野蛮时代》（Brutal Age）是一款风靡全球的SLG类型手机游戏。根据App Annie统计，《野蛮时代》在12个国家取得游戏畅销榜第1，在82个国家取得游戏畅销榜前10。准确了解每个玩家的价值，对游戏的广告投放策略和高效的运营活动（如精准的促销活动和礼包推荐）具有重要意义，有助于给玩家带来更个性化的体验。因此，我们希望能在玩家进入游戏的前期就对于他们的价值进行准确的估算。在这个竞赛里，想请各位选手利用玩家在游戏内前7日的行为数据，预测他们每个人在45日内的付费总金额。  \n",
    "> **提示：**类似于此部分的引用部分可以提供关于如何浏览和使用 iPython notebook 的实用说明。\n",
    "\n",
    "# 开始\n",
    "处理train和test数据集，通过将数去读取生成pandas dataframe，对数据做进一步的操作和和可视化分析\n",
    "运行以下代码单元格，加载我们的数据并使用 `.head()` 函数显示前几个条目（乘客）以进行检查。\n",
    "> **提示：**你可以通过点击单元格和使用键盘快捷键 **Shift + Enter** 或 **Shift + Return** 运行代码单元格。此外，在选中单元格后还可以使用工具栏中的**播放**按钮执行代码单元格。双击 Markdown 单元格（如下所示的文本单元格）即可进行编辑，并使用相同的快捷键保存。[Markdown](http://daringfireball.net/projects/markdown/syntax) 使你能够编写易于阅读的纯文本文件，这些文件可以转换为 HTML。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the dataset\n",
    "train_path = './tap_fun_train.csv'\n",
    "test_path = './tap_fun_test.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "display(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 字段对应描述\n",
    "\n",
    "|字段名|字段解释|数据时间|变量性质|\n",
    "| ---- | ------ | ------ |--------|\n",
    "|user_id|玩家唯一ID|永久|ID\n",
    "|register_time|玩家注册时间|永久|自变量\n",
    "|wood_add_value|木头获取数量|前七日|自变量\n",
    "|wood_reduce_value|木头消耗数量|前七日|自变量\n",
    "|stone_add_value|石头获取数量|前七日|自变量\n",
    "|stone_reduce_value|石头消耗数量|前七日|自变量\n",
    "|ivory_add_value|象牙获取数量|前七日|自变量\n",
    "|ivory_reduce_value|象牙消耗数量|前七日|自变量\n",
    "|meat_add_value|肉获取数量|前七日|自变量\n",
    "|meat_reduce_value|肉消耗数量|前七日|自变量\n",
    "|magic_add_value|魔法获取数量|前七日|自变量\n",
    "|magic_reduce_value|魔法消耗数量|前七日|自变量\n",
    "|infantry_add_value|勇士招募数量|前七日|自变量\n",
    "|infantry_reduce_value|勇士损失数量|前七日|自变量\n",
    "|cavalry_add_value|驯兽师招募数量|前七日|自变量\n",
    "|cavalry_reduce_value|驯兽师损失数量|前七日|自变量\n",
    "|shaman_add_value|萨满招募数量|前七日|自变量\n",
    "|shaman_reduce_value|萨满损失数量|前七日|自变量\n",
    "|wound_infantry_add_value|勇士伤兵产生数量|前七日|自变量\n",
    "|wound_infantry_reduce_value|勇士伤兵恢复数量|前七日|自变量\n",
    "|wound_cavalry_add_value|驯兽师伤兵产生数量|前七日|自变量\n",
    "|wound_cavalry_reduce_value|驯兽师伤兵恢复数量|前七日|自变量\n",
    "|wound_shaman_add_value|萨满伤兵产生数量|前七日|自变量\n",
    "|wound_shaman_reduce_value|萨满伤兵恢复数量|前七日|自变量\n",
    "|general_acceleration_add_value|通用加速获取数量|前七日|自变量\n",
    "|general_acceleration_reduce_value|通用加速使用数量|前七日|自变量\n",
    "|building_acceleration_add_value|建筑加速获取数量|前七日|自变量\n",
    "|building_acceleration_reduce_value|建筑加速使用数量|前七日|自变量\n",
    "|reaserch_acceleration_add_value|科研加速获取数量|前七日|自变量\n",
    "|reaserch_acceleration_reduce_value|科研加速使用数量|前七日|自变量\n",
    "|training_acceleration_add_value|训练加速获取数量|前七日|自变量\n",
    "|training_acceleration_reduce_value|训练加速使用数量|前七日|自变量\n",
    "|treatment_acceleraion_add_value|治疗加速获取数量|前七日|自变量\n",
    "|treatment_acceleration_reduce_value|治疗加速使用数量|前七日|自变量\n",
    "|bd_training_hut_level|建筑：士兵小屋等级|前七日|自变量\n",
    "|bd_healing_lodge_level|建筑：治疗小井等级|前七日|自变量\n",
    "|bd_stronghold_level|建筑：要塞等级|前七日|自变量\n",
    "|bd_outpost_portal_level|建筑：据点传送门等级|前七日|自变量\n",
    "|bd_barrack_level|建筑：兵营等级|前七日|自变量\n",
    "|bd_healing_spring_level|建筑：治疗之泉等级|前七日|自变量\n",
    "|bd_dolmen_level|建筑：智慧神庙等级|前七日|自变量\n",
    "|bd_guest_cavern_level|建筑：联盟大厅等级|前七日|自变量\n",
    "|bd_warehouse_level|建筑：仓库等级|前七日|自变量\n",
    "|bd_watchtower_level|建筑：瞭望塔等级|前七日|自变量\n",
    "|bd_magic_coin_tree_level|建筑：魔法幸运树等级|前七日|自变量\n",
    "|bd_hall_of_war_level|建筑：战争大厅等级|前七日|自变量\n",
    "|bd_market_level|建筑：联盟货车等级|前七日|自变量\n",
    "|bd_hero_gacha_level|建筑：占卜台等级|前七日|自变量\n",
    "|bd_hero_strengthen_level|建筑：祭坛等级|前七日|自变量\n",
    "|bd_hero_pve_level|建筑：冒险传送门等级|前七日|自变量\n",
    "|sr_scout_level|科研：侦查等级|前七日|自变量\n",
    "|sr_training_speed_level|科研：训练速度等级|前七日|自变量\n",
    "|sr_infantry_tier_2_level|科研：守护者|前七日|自变量\n",
    "|sr_cavalry_tier_2_level|科研：巨兽驯兽师|前七日|自变量\n",
    "|sr_shaman_tier_2_level|科研：吟唱者|前七日|自变量\n",
    "|sr_infantry_atk_level|科研：勇士攻击|前七日|自变量\n",
    "|sr_cavalry_atk_level|科研：驯兽师攻击|前七日|自变量\n",
    "|sr_shaman_atk_level|科研：萨满攻击|前七日|自变量\n",
    "|sr_infantry_tier_3_level|科研：战斗大师|前七日|自变量\n",
    "|sr_cavalry_tier_3_level|科研：高阶巨兽骑兵|前七日|自变量\n",
    "|sr_shaman_tier_3_level|科研：图腾大师|前七日|自变量\n",
    "|sr_troop_defense_level|科研：部队防御|前七日|自变量\n",
    "|sr_infantry_def_level|科研：勇士防御|前七日|自变量\n",
    "|sr_cavalry_def_level|科研：驯兽师防御|前七日|自变量\n",
    "|sr_shaman_def_level|科研：萨满防御|前七日|自变量\n",
    "|sr_infantry_hp_level|科研：勇士生命|前七日|自变量\n",
    "|sr_cavalry_hp_level|科研：驯兽师生命|前七日|自变量\n",
    "|sr_shaman_hp_level|科研：萨满生命|前七日|自变量\n",
    "|sr_infantry_tier_4_level|科研：狂战士|前七日|自变量\n",
    "|sr_cavalry_tier_4_level|科研：龙骑兵|前七日|自变量\n",
    "|sr_shaman_tier_4_level|科研：神谕者|前七日|自变量\n",
    "|sr_troop_attack_level|科研：部队攻击|前七日|自变量\n",
    "|sr_construction_speed_level|科研：建造速度|前七日|自变量\n",
    "|sr_hide_storage_level|科研：资源保护|前七日|自变量\n",
    "|sr_troop_consumption_level|科研：部队消耗|前七日|自变量\n",
    "|sr_rss_a_prod_levell|科研：木材生产|前七日|自变量\n",
    "|sr_rss_b_prod_level|科研：石头生产|前七日|自变量\n",
    "|sr_rss_c_prod_level|科研：象牙生产|前七日|自变量\n",
    "|sr_rss_d_prod_level|科研：肉类生产|前七日|自变量\n",
    "|sr_rss_a_gather_level|科研：木材采集|前七日|自变量\n",
    "|sr_rss_b_gather_level|科研：石头采集|前七日|自变量\n",
    "|sr_rss_c_gather_level|科研：象牙采集|前七日|自变量\n",
    "|sr_rss_d_gather_level|科研：肉类生产|前七日|自变量\n",
    "|sr_troop_load_level|科研：部队负重|前七日|自变量\n",
    "|sr_rss_e_gather_level|科研：魔法采集|前七日|自变量\n",
    "|sr_rss_e_prod_level|科研：魔法生产|前七日|自变量\n",
    "|sr_outpost_durability_level|科研：据点耐久|前七日|自变量\n",
    "|sr_outpost_tier_2_level|科研：据点二|前七日|自变量\n",
    "|sr_healing_space_level|科研：医院容量|前七日|自变量\n",
    "|sr_gathering_hunter_buff_level|科研：领土采集奖励|前七日|自变量\n",
    "|sr_healing_speed_level|科研：治疗速度|前七日|自变量\n",
    "|sr_outpost_tier_3_level|科研：据点三|前七日|自变量\n",
    "|sr_alliance_march_speed_level|科研：联盟行军速度|前七日|自变量\n",
    "|sr_pvp_march_speed_level|科研：战斗行军速度|前七日|自变量\n",
    "|sr_gathering_march_speed_level|科研：采集行军速度|前七日|自变量\n",
    "|sr_outpost_tier_4_level|科研：据点四|前七日|自变量\n",
    "|sr_guest_troop_capacity_level|科研：增援部队容量|前七日|自变量\n",
    "|sr_march_size_level|科研：行军大小|前七日|自变量\n",
    "|sr_rss_help_bonus_level|科研：资源帮助容量|前七日|自变量\n",
    "|pvp_battle_count|PVP次数|前七日|自变量\n",
    "|pvp_lanch_count|主动发起PVP次数|前七日|自变量\n",
    "|pvp_win_count|PVP胜利次数|前七日|自变量\n",
    "|pve_battle_count|PVE次数|前七日|自变量\n",
    "|pve_lanch_count|主动发起PVE次数|前七日|自变量\n",
    "|pve_win_count|PVE胜利次数|前七日|自变量\n",
    "|avg_online_minutes|在线时长|前七日|自变量\n",
    "|pay_price|付费金额|前七日|自变量\n",
    "|pay_count|付费次数|前七日|自变量\n",
    "|prediction_pay_price|45日付费金额|前45日|因变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练样本集和测试集的数据量\n",
    "print(train_df.shape[0],test_df.shape[0])\n",
    "\n",
    "## 获取feature column\n",
    "feature_names = test_df.columns.values.tolist()\n",
    "target_name = 'prediction_pay_price'\n",
    "\n",
    "## 确保user_id是unique的\n",
    "assert train_df['user_id'].nunique() ==train_df['user_id'].shape[0]\n",
    "assert test_df['user_id'].nunique() ==test_df['user_id'].shape[0]\n",
    "\n",
    "## 确保train数据中的user_id不在test中存在\n",
    "assert len(set(test_df['user_id'].unique()).intersection(set(train_df['user_id'].unique())))==0\n",
    "\n",
    "## user_id是唯一标识\n",
    "feature_names.remove('user_id')\n",
    "\n",
    "## 先抛开时间检验数据的特征，事实上时间也能够继续划分，提取日期特征\n",
    "feature_names.remove('register_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务分析\n",
    "首先这是一个回归问题，因此我们需要对y做出一些分析，并且我们可以通过画图观察标签的分布，并且对比scaling技巧对于y的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先区分有消费和无消费人群占原始数据的比例\n",
    "unpayed = train_df[train_df[target_name]==0].shape[0]\n",
    "payed = train_df[train_df[target_name]!=0].shape[0]\n",
    "\n",
    "print(u'付费用户占总样本:',float(payed)/train_df.shape[0])\n",
    "print(u'未付费用户占总样本:',float(unpayed)/train_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出绝大部分用户是未付费用户，我们再来看一下付费用户中的数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析一下其中消费群体最大、最小、中位数、平均值等\n",
    "display(train_df[train_df[target_name]!=0][target_name].describe())\n",
    "\n",
    "# 消费金额分布图\n",
    "train_df[train_df[target_name]!=0][target_name].hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除去部分高消费人群，大部分用户的消费金额还是偏低的。因此需要通过将数据取对数，消除scale的影响，观察scale过后的数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析一下对数消费金额其中消费群体最大、最小、中位数、平均值等\n",
    "display(np.log1p(train_df[train_df[target_name]!=0][target_name]).describe())\n",
    "\n",
    "# 对数消费金额分布图\n",
    "np.log1p(train_df[train_df[target_name]!=0][target_name]).hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析一下sqrt消费金额其中消费群体最大、最小、中位数、平均值等\n",
    "display((train_df[train_df[target_name]!=0][target_name]**0.5).describe())\n",
    "\n",
    "# sqrt消费金额分布图\n",
    "(train_df[train_df[target_name]!=0][target_name]**0.5).hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出通过log scaling和sqrt transform，数据的scale差距缩小，但是否要对y进行scale进行训练还需要进一步判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 1\n",
    "如果检测特征之间相互的线性相关系数？\n",
    "\n",
    "通过计算特征的pearson correlation并使用matplotlib的绘图功能，我们可以进一步观察特征之间correlation的程度，并且检查train和test中特征相关性是否一致，我们将通过可视化其相关性矩阵和相关性分布图来进行分析\n",
    "\n",
    "\n",
    "在pandas的DataFrame中，可以使用.corr()函数直接计算一个dataframe中所有特征间相互的特征系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_correlation(data):\n",
    "    \"\"\" using pandas.DataFrame.corr to get correlation between features \"\"\"\n",
    "    corr = None\n",
    "    return corr\n",
    "\n",
    "## keep only features in train_df\n",
    "train_correlations = get_correlation(train_df.drop([target_name], axis=1))\n",
    "test_correlations = get_correlation(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap   相关性热点矩阵\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of train Features',y=1.05,size=15)\n",
    "sns.heatmap(train_correlations,linewidths=0.1,vmax=1.0,\n",
    "            square=True,linecolor='white',annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of test Features',y=1.05,size=15)\n",
    "sns.heatmap(test_correlations,linewidths=0.1,vmax=1.0,\n",
    "            square=True,linecolor='white',annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以从图中看出，拥有相同前缀的特征倾向于更加correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化train test数据特征correlation的分布\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "train_correlations_dist = train_correlations.values.flatten()\n",
    "train_correlations_dist = train_correlations_dist[train_correlations_dist!=1]\n",
    "\n",
    "test_correlations_dist = test_correlations.values.flatten()\n",
    "test_correlations_dist = test_correlations_dist[test_correlations_dist!=1]\n",
    "\n",
    "sns.distplot(train_correlations_dist, color=\"Red\", label=\"train\",bins=100)\n",
    "sns.distplot(test_correlations_dist, color=\"Green\", label=\"test\",bins=100)\n",
    "plt.xlabel(\"Correlation values found in train (except 1)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Are there correlations between features?\"); \n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集和测试集中数据的linear correlation的分布在某些地方不太一致，这是一个很有意思的现象，可能需要注意这部分特征在训练过程中的预处理或者后处理\n",
    "产生这种现象的原因可能有很多，比如训练和测试集并非采用相同采样手段得到结果，或者训练和验证集并不是由同一个时间段生成的，或者是由于一些隐变量（做过一次活动）等导致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据关于y的分布 ##\n",
    "我们可以通过可视化的方式，来观察数据点关于y在一个二维平面中的分布\n",
    "通过使用PCA对原始数据降维至2维空间，我们可以简单的画出一个关于降维特征的二维平面图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2\n",
    "如何使用pca进行数据的可视化\n",
    "\n",
    "* pca是一种简单有效的线性降维手段，通常使用pca后观察其component，可以大致了解数据的分布，为此我们需要使用sklearn.decomposition中的PCA类进行降维,同时在计算pca前还需要对数据使用标准化操作。\n",
    "\n",
    "\n",
    "**提示：** 可以使用sklearn中的make_pipeline方法，串联StandardScaler和PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = train_df[:1000000][feature_names].values\n",
    "y = train_df[:1000000][target_name].values\n",
    "\n",
    "\n",
    "def get_pca_decomposition(X):\n",
    "    # using StandardScaler and PCA to get decomposition\n",
    "    X_pca = None\n",
    "    return X_pca\n",
    "\n",
    "X_pca = get_pca_decomposition(X)\n",
    "\n",
    "\n",
    "## 采样，使付费和未付费数量一致\n",
    "y_pay = y[y>0]\n",
    "y_unpay = y[y==0][:len(y_pay)]\n",
    "X_pca_pay = X_pca[[y>0]]\n",
    "X_pca_unpay = X_pca[[y==0]][:len(y_pay)]\n",
    "\n",
    "## 付费用户pca\n",
    "plt.scatter(X_pca_pay[:,0],X_pca_pay[:,1],c=y_pay)\n",
    "plt.show()\n",
    "\n",
    "## 未付费用户pca\n",
    "plt.scatter(X_pca_unpay[:,0],X_pca_unpay[:,1],c=y_unpay)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中付费用户和未付费用户的区分还是非常明显的，并且由于样本的不均衡性，对于非付费用户的误分可能会导致精度的下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top feature 的重要性 ##\n",
    "为了更好地观测数据，我们可以使用模型进行简单的训练，基于训练出的模型得到特征的重要性排序\n",
    "为了使得训练更简单快速，我们可以使用随即森林作为尝试模型进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题 3\n",
    "如果搭建训练一个随机森林，并且预测结果得到其准确度\n",
    "*  采用sklearn.ensemble 中的RandomForestRegressor进行回归预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "## 使用少量数据进行训练，加快速度\n",
    "X = train_df[:500000][feature_names].values\n",
    "\n",
    "y = train_df[:500000][target_name].values\n",
    "\n",
    "## 切分数据集得到train validation\n",
    "kf = KFold(n_splits=5,random_state=1,shuffle=True).split(X)\n",
    "ind_tr,ind_te = next(kf)\n",
    "X_train = X[ind_tr]\n",
    "X_test = X[ind_te]\n",
    "\n",
    "y_train = y[ind_tr]\n",
    "y_test = y[ind_te]\n",
    "\n",
    "\n",
    "def train_random_forest(X_train,y_train):\n",
    "    rf = None\n",
    "    return rf\n",
    "\n",
    "\n",
    "rf = train_random_forest(X_train,y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "## 通过mse计算rmse\n",
    "score = mse(y_test,y_pred)**0.5\n",
    "\n",
    "print('rf score:%s'%score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们以通过训练得到的模型，查看重要性最高的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设定查看前10个最重要的特征\n",
    "n_top = 10\n",
    "importances = rf.feature_importances_\n",
    "idx = np.argsort(importances)[::-1][0:n_top]\n",
    "feature_names = np.array(feature_names)\n",
    "\n",
    "## 可视化特征重要性\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x=feature_names[idx], y=importances[idx]);\n",
    "plt.title(\"What are the top important features to start with?\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们可以更进一步对这些特征进行分析与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(n_top,2,figsize=(20,5*n_top))\n",
    "for n in range(n_top):\n",
    "    sns.distplot((train_df.loc[:, feature_names[idx][n]]), ax=ax[n,0], color=\"red\", norm_hist=True,bins=50)\n",
    "    sns.distplot((test_df.loc[:, feature_names[idx][n]]), ax=ax[n,1], color=\"Mediumseagreen\", norm_hist=True,bins=50)\n",
    "    ax[n,0].set_title(\"Train {}\".format(feature_names[idx][n]))\n",
    "    ax[n,1].set_title(\"Test {}\".format(feature_names[idx][n]))\n",
    "    ax[n,0].set_xlabel(\"\")\n",
    "    ax[n,1].set_xlabel(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化partial dependency ###\n",
    "无论哪一个特征，绝大部分特征值都集中在0或者靠近0的部分，是否可以说明这部分玩家因为在游戏初期投入较少或者不投入，而导致在未来45天也会倾向于不投入，针对这个问题，我们可以采用验证partial_dependency的方法来进行假设的检验,通过观察partial dependency图，我们可以大致理解这些特征中数值范围对于预测45天消费金额的“贡献”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=1,shuffle=True).split(train_df[:500000].values)\n",
    "ind_tr_gbdt,ind_te_gbdt = next(kf)\n",
    "\n",
    "\n",
    "top_features = feature_names[idx]\n",
    "\n",
    "X_train_top = train_df[:500000].iloc[ind_tr_gbdt][top_features].values\n",
    "X_test_top = train_df[:500000].iloc[ind_te_gbdt][top_features].values\n",
    "\n",
    "y_train_top = train_df[:500000].iloc[ind_tr_gbdt][target_name].values\n",
    "y_test_top = train_df[:500000].iloc[ind_te_gbdt][target_name].values\n",
    "\n",
    "clf = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf.fit(X_train_top, y_train_top)\n",
    "print(\" done.\")\n",
    "y_pred_top = clf.predict(X_test_top)\n",
    "\n",
    "score = mse(y_test_top,y_pred_top)**0.5\n",
    "\n",
    "print('gbdt score:%s'%score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题4.1\n",
    "画出partial dependency的图像\n",
    "\n",
    "* 提示：[plot_partial_dependence][1]相关方法可以参照sklearn文档以及其中的案例\n",
    "[1]: (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html#sklearn.ensemble.partial_dependence.plot_partial_dependence） \"sklearn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Convenience plot with ``partial_dependence_plots``')\n",
    "\n",
    "features = np.arange(len(top_features))\n",
    "for i in range(len(top_features)):\n",
    "    ## fill the parameter and generate figures\n",
    "    fig, axs = plot_partial_dependence()\n",
    "    fig.suptitle('Partial dependence of %s'%top_features[i])\n",
    "    plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据partial dependency plot的结果，我们可以对数据获得更进一步的确认，以pay_price为例，我们可以确定的说，当一个用户在注册初期花费的越多，其在45天内总开销也可能越多\n",
    "通过这种方式，能够更直观的理解特征和标签的关系，并且可以用来检查特征两两之间的组合，我们再来看一个例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题4.2\n",
    "画出特征重要性第一和第二位特征的partial_dependence\n",
    "\n",
    "* 通过设定target_feature = (0,1)，指定对应特征在top_features中的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "target_feature = (0,1)\n",
    "## fill the parameter to draw figure\n",
    "fig, axs = plot_partial_dependence()\n",
    "fig.suptitle('Partial dependence of pay_price and pay_count')\n",
    "plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中图像颜色越深代表贡献越多，因此我们可以观察到其中深色面积是由pay_price和pay_count共同构成，并且可以从两个不同的维度上去理解特征之间交互贡献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scale对回归预测值的影响检测 ##\n",
    "通常情况下，对于金额类回归，对于标签y进行scale处理会有利于提高模型的精度，消除数据标签中存在的异方差，因此我们可以采用相同模型进行对比，此处我们依然使用top10的特征，并且采用相同的gbdt结构和参数进行模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题5.1\n",
    "计算log transformation\n",
    "在计算过程中，遇到变量为0的情况，可以采用+1的方式数据进行平滑操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## log transformation\n",
    "def log_scale(x):\n",
    "    x_scaled = None\n",
    "    return x_scaled\n",
    "\n",
    "y_train_top_log = log_scale(train_df[:500000].iloc[ind_tr_gbdt][target_name].values)\n",
    "y_test_top_log = log_scale(train_df[:500000].iloc[ind_te_gbdt][target_name].values)\n",
    "\n",
    "clf_log = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_log.fit(X_train_top, y_train_top_log)\n",
    "print(\" done.\")\n",
    "y_pred_top_log = clf_log.predict(X_test_top)\n",
    "\n",
    "\n",
    "y_pred_top_log[y_pred_top_log<0]=0\n",
    "score = mse(\n",
    "   (y_test_top),np.expm1(y_pred_top_log))**0.5\n",
    "print('gbdt score:%s'%score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题5.2\n",
    "计算square root transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## square root transformation\n",
    "\n",
    "def sqrt_scale(x):\n",
    "    x_scaled = None\n",
    "    return x_scaled\n",
    "\n",
    "y_train_top_sqrt = sqrt_scale(train_df[:500000].iloc[ind_tr_gbdt][target_name].values)\n",
    "y_test_top_sqrt = sqrt_scale(train_df[:500000].iloc[ind_te_gbdt][target_name].values)\n",
    "\n",
    "clf_sqrt = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_sqrt.fit(X_train_top, y_train_top_sqrt)\n",
    "print(\" done.\")\n",
    "y_pred_top_sqrt = clf_sqrt.predict(X_test_top)\n",
    "\n",
    "y_pred_top_sqrt[y_pred_top_sqrt<0]=0\n",
    "score = mse(\n",
    "   (y_test_top),(y_pred_top_sqrt)**2)**0.5\n",
    "print('gbdt score:^%s'%score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题5.3\n",
    "计算z-socre transformation(标准化),并且在预测时通过mean和std的值转换回原scale的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## z-socre transformation\n",
    "def z_score(x,mean,std):\n",
    "    x_scaled = None\n",
    "    return x_scaled\n",
    "\n",
    "## convert back scaled data\n",
    "def inverse_z_score(x,mean,std):\n",
    "    x_inversed = None\n",
    "    return x_inversed\n",
    "\n",
    "mean = np.mean(train_df[:500000].iloc[ind_tr_gbdt][target_name].values)\n",
    "std = np.std(train_df[:500000].iloc[ind_tr_gbdt][target_name].values)\n",
    "\n",
    "y_train_top_z = z_score(train_df[:500000].iloc[ind_tr_gbdt][target_name].values,mean,std)\n",
    "y_test_top_z = z_score(train_df[:500000].iloc[ind_te_gbdt][target_name].values,mean,std)\n",
    "\n",
    "clf_z = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_z.fit(X_train_top, y_train_top_z)\n",
    "print(\" done.\")\n",
    "y_pred_top_z = clf_sqrt.predict(X_test_top)\n",
    "\n",
    "score = mse(\n",
    "   (y_test_top),inverse_z_score(y_pred_top_z,mean,std))**0.5\n",
    "print('gbdt score:^%s'%score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们测试了log transformation，sqrt transformation，z-score transformation并且对比其训练结果后发现sqrt transformation的效果好于另外两种。对于log变换还可以采用各种smoothing策略来对输入输出调整:\n",
    "f(x) = log(x+a)+b\n",
    "inverse_f(x) = exp(x-b)-a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的基于知识的特征工程 ##\n",
    "任何一个机器学习的项目都会牵涉到大量特征工程的操作，通常除去数据的预处理和分析的时间外，在这样一个典型的传统机器学习任务中，特征工程将会占用大量的分析，尝试和探索的时间。通过构建新特征，验证假设，并且不断迭代这个步骤，是的模型在优化过程中不断提升。\n",
    "基于知识或者基于数据探索性分析的结果来构造特征，能够使得需要训练的模型获得更好的泛化能力。良好的特征是提升预测结果精度最重要的一环。\n",
    "例如：\n",
    "\n",
    "当我们已经知晓pvp的获胜次数和pvp的参加次数时，其胜率可能是一个更好的组合特征，拥有越高的pvp胜率，很有可能促进用户投入更多的资金来维持自己的游戏体验，因此我们可以基于pvp_win_count和pvp_battle_count来构建新的特征\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题6 计算pvp、pve的胜率生成新特征\n",
    "\n",
    "#### 问题6.1\n",
    "胜率的计算方式为：\n",
    "胜率 = 获胜次数/参与次数\n",
    "\n",
    "* 思考：当count数值作为分母为0时该怎么处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 已知pvp的进行次数和获胜次数，求其比例\n",
    "train_df['pvp_win_ratio'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好地观察数据，在测试效果前我们先查看生成特征关于y的线性相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 首先对生成的特征计算线性相关性\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_pvp_battle = pearsonr(train_df[:500000][target_name],train_df[:500000]['pvp_battle_count'])\n",
    "corr_pvp_vin = pearsonr(train_df[:500000][target_name],train_df[:500000]['pvp_win_count'])\n",
    "print('correlation value of pvp_battle_count and target:',corr_pvp_battle)\n",
    "print('correlation value of pvp_win_count and target:',corr_pvp_vin)\n",
    "\n",
    "corr_ratio = pearsonr(train_df[:500000][target_name],train_df[:500000]['pvp_win_ratio'])\n",
    "print('correlation value of new feature and target:',corr_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们尝试将对比添加特征前和添加特征后的模型表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 添加胜率前的结果\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "target_name = 'prediction_pay_price'\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=1,shuffle=True).split(train_df[:500000].values)\n",
    "ind_tr_gbdt,ind_te_gbdt = next(kf)\n",
    "\n",
    "X_train = train_df[:500000].iloc[ind_tr_gbdt][['pvp_win_count','pvp_battle_count']].values\n",
    "X_test = train_df[:500000].iloc[ind_te_gbdt][['pvp_win_count','pvp_battle_count']].values\n",
    "\n",
    "y_train_top_sqrt = (train_df[:500000].iloc[ind_tr_gbdt][target_name].values)**0.5\n",
    "y_test_top_sqrt = (train_df[:500000].iloc[ind_te_gbdt][target_name].values)**0.5\n",
    "\n",
    "clf_sqrt = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_sqrt.fit(X_train, y_train_top_sqrt)\n",
    "print(\" done.\")\n",
    "y_pred_top_sqrt = clf_sqrt.predict(X_test)\n",
    "\n",
    "y_pred_top_sqrt[y_pred_top_sqrt<0]=0\n",
    "score = mse(\n",
    "   (y_test_top_sqrt)**2,(y_pred_top_sqrt)**2)**0.5\n",
    "print('gbdt score:%s'%score)\n",
    "\n",
    "\n",
    "## 添加胜率后的结果\n",
    "X_train = train_df[:500000].iloc[ind_tr_gbdt][['pvp_win_count','pvp_battle_count','pvp_win_ratio']].values\n",
    "X_test = train_df[:500000].iloc[ind_te_gbdt][['pvp_win_count','pvp_battle_count','pvp_win_ratio']].values\n",
    "\n",
    "y_train_top_sqrt = (train_df[:500000].iloc[ind_tr_gbdt][target_name].values)**0.5\n",
    "y_test_top_sqrt = (train_df[:500000].iloc[ind_te_gbdt][target_name].values)**0.5\n",
    "\n",
    "clf_sqrt = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_sqrt.fit(X_train, y_train_top_sqrt)\n",
    "print(\" done.\")\n",
    "y_pred_top_sqrt = clf_sqrt.predict(X_test)\n",
    "\n",
    "y_pred_top_sqrt[y_pred_top_sqrt<0]=0\n",
    "score = mse(\n",
    "   (y_test_top_sqrt)**2,(y_pred_top_sqrt)**2)**0.5\n",
    "print('gbdt score:%s'%score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我可以从训练中的结果看出，构造了新的特征并加入训练后，模型的效果明显提升\n",
    "通过大量尝试构造特征，生成拥有更强预测能力的特征集能够帮助提升模型训练效果\n",
    "\n",
    "再来尝试一下pve，使用相同的方法构建特征，同样可以带来提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题6.2 计算pve胜率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pve_win_ratio'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 添加胜率前的结果\n",
    "## 已知pve的进行次数和获胜次数，求其比例\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "\n",
    "target_name = 'prediction_pay_price'\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=1,shuffle=True).split(train_df[:500000].values)\n",
    "ind_tr_gbdt,ind_te_gbdt = next(kf)\n",
    "\n",
    "X_train = train_df[:500000].iloc[ind_tr_gbdt][['pve_win_count','pve_battle_count']].values\n",
    "X_test = train_df[:500000].iloc[ind_te_gbdt][['pve_win_count','pve_battle_count']].values\n",
    "\n",
    "y_train_top_sqrt = (train_df[:500000].iloc[ind_tr_gbdt][target_name].values)**0.5\n",
    "y_test_top_sqrt = (train_df[:500000].iloc[ind_te_gbdt][target_name].values)**0.5\n",
    "\n",
    "clf_sqrt = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_sqrt.fit(X_train, y_train_top_sqrt)\n",
    "print(\" done.\")\n",
    "y_pred_top_sqrt = clf_sqrt.predict(X_test)\n",
    "\n",
    "y_pred_top_sqrt[y_pred_top_sqrt<0]=0\n",
    "score = mse(\n",
    "   (y_test_top_sqrt)**2,(y_pred_top_sqrt)**2)**0.5\n",
    "print('gbdt score:%s'%score)\n",
    "\n",
    "\n",
    "## 添加胜率后的结果\n",
    "X_train = train_df[:500000].iloc[ind_tr_gbdt][['pve_win_count','pve_battle_count','pve_win_ratio']].values\n",
    "X_test = train_df[:500000].iloc[ind_te_gbdt][['pve_win_count','pve_battle_count','pve_win_ratio']].values\n",
    "\n",
    "y_train_top_sqrt = (train_df[:500000].iloc[ind_tr_gbdt][target_name].values)**0.5\n",
    "y_test_top_sqrt = (train_df[:500000].iloc[ind_te_gbdt][target_name].values)**0.5\n",
    "\n",
    "clf_sqrt = GradientBoostingRegressor(n_estimators=100, max_depth=8,\n",
    "                                    learning_rate=0.1,\n",
    "                                    random_state=1,verbose=1)\n",
    "\n",
    "clf_sqrt.fit(X_train, y_train_top_sqrt)\n",
    "print(\" done.\")\n",
    "y_pred_top_sqrt = clf_sqrt.predict(X_test)\n",
    "\n",
    "y_pred_top_sqrt[y_pred_top_sqrt<0]=0\n",
    "score = mse(\n",
    "   (y_test_top_sqrt)**2,(y_pred_top_sqrt)**2)**0.5\n",
    "print('gbdt score:%s'%score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "machine learning通常情况下并不是上手直接将数据转换成特征进行模型训练\n",
    "在做任何训练之前，最好通过各种eda手段对现有数据的一些特性进行分析，掌握特征的一些概况。真实场景环境下给定的数据比学习项目中遇到的更加复杂，数据的缺失、分布、特征的构造等都会对模型的训练以及预测造成非常重大的影响。\n",
    "\n",
    "最好的方式是通过不断迭代自己的探索性分析脚本，从数据中发现更多规律，从而从整体角度理解数据。然后再根据发现的一些规律和特点，进行特征工程与模型训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
